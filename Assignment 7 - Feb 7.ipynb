{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping, Optional, Set, Callable\n",
    "import numpy as np\n",
    "from utils.generic_typevars import S, A\n",
    "from utils.mp_funcs import get_rv_gen_func_single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write code for the interface for tabular RL algorithms. The core of this interface should be a mapping from a (state, action) pair to a sampling of the (next state, reward) pair. It is important that this interface doesn't present the state-transition probability model or the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDPforRLTab:\n",
    "    '''\n",
    "        First define the MDP class\n",
    "    '''\n",
    "    def __init__(self, policy, actions: Mapping[S, Set[A]], terminal_states: Set[S], state_reward_gen_dict, gamma: float):\n",
    "        self.policy = policy\n",
    "        self.actions = actions\n",
    "        self.terminal_states = terminal_states\n",
    "        self.state_reward_gen_dict = state_reward_gen_dict # a dictionary of functions that generate the next state and reward\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def get_actions(self, s):\n",
    "        return self.actions[s]\n",
    "    \n",
    "    def get_terminal_states(self, s):\n",
    "        return s in self.terminal_states\n",
    "    \n",
    "    def get_state_reward_gen_func(self, s, a):\n",
    "        return self.state_reward_gen_dict[s][a]()\n",
    "    \n",
    "    def init_state_gen(self):\n",
    "        dic = {}\n",
    "        for s in self.actions.keys():\n",
    "            dic[s] = 1. / len(self.actions)\n",
    "        return get_rv_gen_func_single(dic)\n",
    "    \n",
    "    def init_state_action_gen(self):\n",
    "        dic = {}\n",
    "        for s, v1 in self.actions.items():\n",
    "            for a in v1:\n",
    "                dic[(s, a)] = 1. / sum(len(v) for v in self.actions.values())\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLTabInterface:\n",
    "    '''\n",
    "    A model-free RL interface that does not need the state-transition probability model or the reward model\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, mdp_for_rl_tab: MDPforRLTab, exploring_start: bool, softmax: bool, epsilon: float, \n",
    "                 epsilon_half_life: float, num_episodes: int, max_steps: int):\n",
    "\n",
    "        self.mdp = mdp_rep_for_rl\n",
    "\n",
    "    # get a state-action dictionary\n",
    "    def get_actions(self) -> Mapping[S, Set[A]]:\n",
    "        return self.mdp.actions\n",
    "    \n",
    "    # check whether a state is a terminal state\n",
    "    def get_terminal_states(self, s) -> bool:\n",
    "        return self.mdp.get_terminal_states(s)\n",
    "    \n",
    "    # get a sampling of the (next state, reward) pair\n",
    "    def get_next_pair(self, s, a):\n",
    "        next_state, reward = self.mdp.get_state_reward_gen_func(s, a)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement any tabular Monte-Carlo algorithm for Value Function prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement First visit Monte-Carlo algorithm, where the first time-step t that state s is visited in an episode,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helper_funcs import get_returns_from_rewards_terminating\n",
    "from utils.helper_funcs import get_returns_from_rewards_non_terminating\n",
    "\n",
    "class MonteCarlo:\n",
    "    '''\n",
    "        First visit Monte-Carlo algorithm\n",
    "    '''\n",
    "    def __init__(self, mdp: MDPforRLTab, num_episodes: int, max_steps: int):        \n",
    "        self.mdp = mdp\n",
    "        self.max_steps = max_steps\n",
    "        self.num_episodes=num_episodes\n",
    "\n",
    "    def generateEpisode(self, start_state, start_action = None):\n",
    "        '''\n",
    "        generate a single episode\n",
    "        '''\n",
    "        res = []\n",
    "        state = start_state\n",
    "        steps = 0\n",
    "        visited = set()\n",
    "        act_gen_dict = {s: get_rv_gen_func_single(self.mdp.policy[s]) for s in self.mdp.actions.keys()}\n",
    "\n",
    "        while True:\n",
    "            # check whether the state has been visited\n",
    "            first = state not in visited\n",
    "            visited.add(state)\n",
    "            action = start_action if (start_action and steps == 0) else act_gen_dict[state]\n",
    "            next_state, reward = self.mdp.state_reward_gen_dict[state][action]()\n",
    "            res.append((state, action, reward, first))\n",
    "            steps += 1\n",
    "            if steps >= self.max_steps or state in self.mdp.terminal_states:\n",
    "                break\n",
    "            state = next_state\n",
    "        return res\n",
    "\n",
    "    def get_value_func_dict(self):\n",
    "        '''\n",
    "        predict value function\n",
    "        '''        \n",
    "        vf_dict = {s: 0.0 for s in self.mdp.actions.keys()}\n",
    "        episodes = 0\n",
    "\n",
    "        while episodes < self.num_episodes:\n",
    "            start_state = self.mdp.init_state_gen()\n",
    "            path = self.generateEpisode(start_state, start_action=None)\n",
    "            \n",
    "            rewards = np.array([x for _, _, x, _ in path])\n",
    "            # if terminal state\n",
    "            if path[-1][0] in self.mdp.terminal_states:\n",
    "                returns = get_returns_from_rewards_terminating(rewards, self.mdp.gamma)\n",
    "            else:\n",
    "                returns = get_returns_from_rewards_non_terminating(rewards, self.mdp.gamma, self.nt_return_eval_steps)\n",
    "            episodes += 1\n",
    "\n",
    "        return vf_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement tabular 1-step TD algorithm for Value Function prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD(mdp: MDPforRLTab, num_episodes, max_steps, gamma, alpha):    \n",
    "    vf_dict = {s: 0.0 for s in mdp.actions.keys()}   \n",
    "    mc = MonteCarlo(mdp, num_episodes, max_steps)\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        start_state = mdp.init_state_gen()\n",
    "        episode = mc.generateEpisode(start_state, start_action = None)\n",
    "        for j in range(max_steps):\n",
    "            cur_state = episode[j][0]\n",
    "            reward = episode[j][2]\n",
    "            # get the next state\n",
    "            if j < max_steps-1:\n",
    "                next_state = episode[j+1][0]\n",
    "            else:\n",
    "                next_state = episode[0][0]\n",
    "            # get the value function\n",
    "            vf_dict[cur_state] += alpha*(reward + gamma*val[s_next] - val[s_cur])\n",
    "\n",
    "    return vf_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the above implementation of Monte-Carlo and TD Value Function prediction algorithms versus DP Policy Evaluation algorithm on an example MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 6.516608108123774, 1: 6.991429305383525, 2: 8.610530839162942}\n"
     ]
    }
   ],
   "source": [
    "from utils.processes.mp_funcs import get_state_reward_gen_dict\n",
    "\n",
    "actions = {0:[0,1,2], 1:[0,1,2], 2:[0,1,2]}\n",
    "transitions =  {0: {\n",
    "            0: {0: 0.3, 1: 0.6, 2: 0.1},\n",
    "            1: {0: 0.2, 1: 0.4, 2: 0.4},\n",
    "            2: {0: 0.1, 1: 0.5, 2: 0.4}\n",
    "        },\n",
    "        1: {\n",
    "            0: {0: 0.4, 1: 0.5, 2: 0.1},\n",
    "            1: {0: 0.2, 1: 0.5, 2: 0.3},\n",
    "            2: {0: 0.3, 1: 0.2, 2: 0.5}\n",
    "        },\n",
    "        2: {\n",
    "            0: {0: 0.3, 1: 0.3, 2: 0.4},\n",
    "            1: {0: 0.2, 1: 0.6, 2: 0.2},\n",
    "            2: {0: 0.3, 1: 0.4, 2: 0.3}\n",
    "        }}\n",
    "rewards = {0: {\n",
    "            0: {0: 1.0, 1: 2.0, 2: 3.0},\n",
    "            1: {0: 2.0, 1: 3.0, 2: 1.0},\n",
    "            2: {0: 3.0, 1: 2.0, 2: 1.0}\n",
    "        },\n",
    "        1: {\n",
    "            0: {0: 2.0, 1: 2.4, 2: 3.5},\n",
    "            1: {0: 4.0, 1: 3.9, 2: 5.0},\n",
    "            2: {0: 3.2, 1: 7.0, 2: -2.0}\n",
    "        },\n",
    "        2: {\n",
    "            0: {0: 4.0, 1: 2.5, 2: -2.0},\n",
    "            1: {0: 3.8, 1: 4.0, 2: 1.6},\n",
    "            2: {0: 3.1, 1: -2.0, 2: 1.5}\n",
    "        }}\n",
    "policy = {\n",
    "        0: {0: 0.5, 1: 0.1, 2: 0.4},\n",
    "        1: {0: 0.4, 2: 0.6},\n",
    "        2: {1: 1.0}\n",
    "    }\n",
    "alpha = 0.1\n",
    "gamma = 1\n",
    "state_reward_gen_dict = get_state_reward_gen_dict(transitions, rewards)\n",
    "num_episodes = 10\n",
    "max_steps = 10\n",
    "\n",
    "mdp = MDPforRLTab(policy, actions, set(), state_reward_gen_dict, gamma)\n",
    "print(TD(mdp, num_episodes, max_steps, gamma, alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prove that fixed learning rate (step size alpha) for MC is equivalent to an exponentially decaying average of episode returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $k^{th}$ update,\n",
    "$$ \\begin{split}\n",
    "V^k(S_t) &= V^{k-1}(S_t) + \\alpha (G^{k-1}_t -V^{k-1}(S_t) )\\\\\n",
    "&= (1-\\alpha)V^{k-1}(S_t)+\\alpha G^{k-1}_t \\\\\n",
    "&= (1-\\alpha)((1-\\alpha)V^{k-2}(S_t)+ \\alpha G^{k-2}_t)+ \\alpha G^{k-1}_t \\\\\n",
    "&= \\alpha G^{k-1}_t + (1-\\alpha)\\alpha G^{k-2}_t + ... +(1-\\alpha)^{k-1} \\alpha G^{0}_t + (1-\\alpha)^{k} V^{0}(S_t)\n",
    "\\end{split}$$ \n",
    "Therefore, it is equivalent to an exponentially decaying average of episode returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
