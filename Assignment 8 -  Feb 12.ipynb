{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping, Optional, Set, Callable\n",
    "import numpy as np\n",
    "from utils.generic_typevars import S, A\n",
    "from utils.mp_funcs import get_rv_gen_func_single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Forward-View TD(Lambda) algorithm for Value Function Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDPforRLTab:\n",
    "    '''\n",
    "        First define the MDP class\n",
    "    '''\n",
    "    def __init__(self, policy, actions: Mapping[S, Set[A]], terminal_states: Set[S], state_reward_gen_dict, gamma: float):\n",
    "        self.policy = policy\n",
    "        self.actions = actions\n",
    "        self.terminal_states = terminal_states\n",
    "        self.state_reward_gen_dict = state_reward_gen_dict # a dictionary of functions that generate the next state and reward\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def get_actions(self, s):\n",
    "        return self.actions[s]\n",
    "    \n",
    "    def get_terminal_states(self, s):\n",
    "        return s in self.terminal_states\n",
    "    \n",
    "    def get_state_reward_gen_func(self, s, a):\n",
    "        return self.state_reward_gen_dict[s][a]()\n",
    "    \n",
    "    def init_state_gen(self):\n",
    "        dic = {}\n",
    "        for s in self.actions.keys():\n",
    "            dic[s] = 1. / len(self.actions)\n",
    "        return get_rv_gen_func_single(dic)\n",
    "    \n",
    "    def init_state_action_gen(self):\n",
    "        dic = {}\n",
    "        for s, v1 in self.actions.items():\n",
    "            for a in v1:\n",
    "                dic[(s, a)] = 1. / sum(len(v) for v in self.actions.values())\n",
    "                \n",
    "\n",
    "class RLTabInterface:\n",
    "    '''\n",
    "    A model-free RL interface that does not need the state-transition probability model or the reward model\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, mdp_for_rl_tab: MDPforRLTab, exploring_start: bool, softmax: bool, epsilon: float, \n",
    "                 epsilon_half_life: float, num_episodes: int, max_steps: int):\n",
    "\n",
    "        self.mdp = mdp_rep_for_rl\n",
    "\n",
    "    # get a state-action dictionary\n",
    "    def get_actions(self) -> Mapping[S, Set[A]]:\n",
    "        return self.mdp.actions\n",
    "    \n",
    "    # check whether a state is a terminal state\n",
    "    def get_terminal_states(self, s) -> bool:\n",
    "        return self.mdp.get_terminal_states(s)\n",
    "    \n",
    "    # get a sampling of the (next state, reward) pair\n",
    "    def get_next_pair(self, s, a):\n",
    "        next_state, reward = self.mdp.get_state_reward_gen_func(s, a)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateEpisode(mdp: RLTabInterface, num_episode: int, max_steps: int, get_action: Callable[[int], int]):\n",
    "    '''\n",
    "    generate episodes \n",
    "    '''\n",
    "    paths = []\n",
    "    for i in range(num_episode):\n",
    "        path = []\n",
    "        cur_state = mdp.init_state_gen()\n",
    "        action = get_action(cur_state)\n",
    "        next_state, reward = mdp.get_state_reward_gen_func(cur_state, action)\n",
    "        path.append((cur_state, action, reward))\n",
    "        for j in range(max_steps):\n",
    "            cur_state = next_state\n",
    "            action = get_action(cur_state)\n",
    "            next_state, reward = mdp.get_state_reward_gen_func(cur_state, action)\n",
    "            path.append((cur_state, action, reward))\n",
    "        paths.append(path)\n",
    "    \n",
    "    return paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foward_pred(mdp: RLTabInterface, num_episode: int, max_steps: int, alpha: float, gamma: float, lamb: float, \n",
    "                 get_action: Callable[[int], int]):\n",
    "    '''\n",
    "    Forward-View TD(Lambda) algorithm for Value Function Prediction\n",
    "    '''\n",
    "    \n",
    "    paths = generateEpisode(mdp, num_episode, max_steps, get_action)\n",
    "    \n",
    "    vf_pred = {s: 0.0 for s in mdp.actions.keys()}\n",
    "    for i in range(num_episode):\n",
    "        path = np.zeros(max_steps)\n",
    "        for j in range(max_steps):\n",
    "            for k in range(j, max_steps):\n",
    "                path[k] += gamma ** j * paths[i][j][2]\n",
    "            path[j] += gamma ** (j+1) * vf_pred(paths[i][j][0])\n",
    "        lamb_ = 0\n",
    "        for j in range(max_steps):\n",
    "            lamb_ += (1-lamb) * lamb ** j * path[j]\n",
    "        # update the value function\n",
    "        vf_pred[paths[i][0][0]] = vf_pred[paths[i][0][0]] + alpha * (lamb_ - vf_pred[paths[i][0][0]])\n",
    "    return vf_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Backward View TD(Lambda), i.e., Eligibility Traces algorithm for Value Function Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pred(mdp: RLTabInterface, num_episode: int, max_steps: int, alpha: float, gamma: float, lamb: float, \n",
    "                   get_action: Callable[[int], int]):\n",
    "    '''\n",
    "    Backward-View TD(Lambda) algorithm for Value Function Prediction\n",
    "    '''        \n",
    "    vf_pred = np.zeros(len(tb_rl.get_states()))\n",
    "    e_t = np.zeros(len(tb_rl.get_states()))\n",
    "    \n",
    "    for i in range(num_episode):\n",
    "        cur_state = mdp.init_state_gen()\n",
    "        for j in range(max_steps):\n",
    "            action = get_action(cur_state)\n",
    "            next_state, reward = mdp.get_state_reward_gen_func(cur_state, action)\n",
    "            e_t *= lamb * gamma\n",
    "            e_t[cur_state] += 1.0\n",
    "            err = reward + gamma * vf_pred[s_next] - vf_pred[cur_state]\n",
    "            vf_pred[cur_state] += alpha*(reward + gamma*vf_pred[next_state] - vf_pred[cur_state])\n",
    "            cur_state = next_state\n",
    "            vf_pred += alpha * err * e_t\n",
    "\n",
    "    return vf_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
