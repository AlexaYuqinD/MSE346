{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping, Set, Sequence\n",
    "from utils.generic_typevars import S, A\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write code for Policy Evaluation (tabular) algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: evaluate a given policy $\\pi$ \n",
    "\n",
    "Solution: iterative application of Bellman expectation backup.\n",
    "\n",
    "$$v^{k+1} = R^{\\pi} + \\gamma P^{\\pi} v^k$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MP:\n",
    "    \"\"\"A class representing a Markov Process\"\"\"\n",
    "    def __init__(self, transitions: Mapping[S, Mapping[S, float]]):\n",
    "        \"\"\"transitions: a dictionary of dictionaries that stores the transition matrix\"\"\"\n",
    "        self.all_states_list = list(transitions.keys()) # a list that store the names of all states\n",
    "        self.transitions = transitions # a dictionary of dictionaries that stores the transition matrix\n",
    "        \n",
    "    def get_all_states(self):\n",
    "        return self.all_states_list\n",
    "        \n",
    "    def get_transition(self, s: S):\n",
    "        try:\n",
    "            return self.transitions[s]\n",
    "        except ValueError:\n",
    "            print(\"Invalid state!\")\n",
    "            \n",
    "    def get_trans_mat(self):\n",
    "        length = len(self.all_states_list)\n",
    "        trans_mat = np.zeros((length, length))\n",
    "        for i in range(length):\n",
    "            for j in range(length):\n",
    "                trans_mat[i, j] = self.transitions[self.all_states_list[i]].get(self.all_states_list[j], 0)\n",
    "        return trans_mat\n",
    "            \n",
    "    def get_stationary_dist(self):\n",
    "        trans_mat = self.get_trans_mat()\n",
    "        eig_val, eig_vec = np.linalg.eig(trans_mat.T)\n",
    "        res = np.array(eig_vec[:, np.where(np.abs(eig_val- 1.) < 1e-8)[0][0]])\n",
    "        res /= np.sum(res)    \n",
    "        return res\n",
    "\n",
    "\n",
    "class MRP(MP):\n",
    "    \"\"\"A class representing a Markov Reward Process\"\"\"\n",
    "    def __init__(self, transitions: Mapping[S, Mapping[S, float]], rewards: Mapping[S, float], gamma: float):\n",
    "        super().__init__(transitions)\n",
    "        self.rewards = rewards\n",
    "        self.reward_vec = self.get_rewards_vec()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def get_reward(self, s: S):\n",
    "        try:\n",
    "            return self.rewards[s]\n",
    "        except ValueError:\n",
    "            print(\"Invalid state!\")\n",
    "            \n",
    "    def get_rewards_vec(self):\n",
    "        length = len(self.all_states_list)\n",
    "        reward_vec = np.zeros(length)\n",
    "        for i in range(length):\n",
    "            reward_vec[i] = self.rewards[self.all_states_list[i]]\n",
    "        return reward_vec             \n",
    "\n",
    "        \n",
    "class MDP():\n",
    "    \"\"\"A class representing a Markov Decision Process\"\"\"\n",
    "    def __init__(self, transitions: Mapping[S, Mapping[A, Mapping[S, float]]], rewards: Mapping[S, Mapping[A, float]], gamma):\n",
    "        self.all_states_list = list(transitions.keys()) \n",
    "        self.all_actions_list = self.get_action_list()\n",
    "        self.transitions = transitions\n",
    "        self.actions = self.get_actions()\n",
    "        self.rewards = rewards\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def get_all_states(self):\n",
    "        return self.all_states_list\n",
    "    \n",
    "    def get_actions(self) ->  Mapping[S, Set[A]]:\n",
    "        action_dict = {}\n",
    "        for s in self.all_states_list:\n",
    "            action_dict[s] = set()\n",
    "        for s1, v1 in self.transitions.items():\n",
    "            for a, v2 in v1.items():\n",
    "                action_dict[s1].add(a)\n",
    "        return action_dict\n",
    "    \n",
    "    def get_action_list(self):\n",
    "        action_list = set()\n",
    "        for s in self.all_states_list:\n",
    "            for a in self.get_actions[s]:\n",
    "                action_list.add(a)\n",
    "        return list(action_list)\n",
    "        \n",
    "    def get_transition(self, s: S, a: A):\n",
    "        try:\n",
    "            return self.transitions[s][a]\n",
    "        except ValueError:\n",
    "            print(\"Invalid state!\")\n",
    "            \n",
    "    def get_mrp(self, policy: Mapping[S, Mapping[A, float]]):\n",
    "        transitions = {}\n",
    "        rewards = {}\n",
    "        for s1, v1 in self.transitions.items():\n",
    "            transitions[s1] = {}\n",
    "            for a, p in policy[s1].items():\n",
    "                for s2, v2 in v1[a].items():\n",
    "                    transitions[s1][s2] = transitions[s1].get(s2, 0) + p*v2\n",
    "                    \n",
    "        for s1, v1 in self.rewards.items():\n",
    "            rewards[s1] = 0\n",
    "            for a, p in policy[s1].items():\n",
    "                rewards[s1] += p*v1[a]\n",
    "        \n",
    "        return MRP(transitions, rewards, self.gamma)\n",
    "    \n",
    "    def matrix_to_policy(self, policy_matrix) -> Mapping[S, Mapping[A, float]]:\n",
    "        policy = {}\n",
    "        for i in range(len(self.all_states_list)):\n",
    "            s = self.all_states_list[i]\n",
    "            policy[s] = {}\n",
    "            for j in range(len(self.all_actions_list)):\n",
    "                a = self.all_actions_list[j]\n",
    "                policy[s][a] = policy_matrix[i][j]\n",
    "        return policy\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(mdp: MDP, policy: Mapping[S, Mapping[A, float]], thres = 1e-8):\n",
    "    mrp = mdp.get_mrp(policy)\n",
    "    length = len(mrp.all_states_list)\n",
    "    old_v = np.zeros(length)   \n",
    "    new_v = np.zeros(length) \n",
    "    while True:\n",
    "        new_v = np.zeros(length) \n",
    "        for i in range(length):\n",
    "            s1 = mrp.all_states_list[i]\n",
    "            for s2 in mrp.transitions[s1]:\n",
    "                new_v[s1] += mrp.rewards[s1] + mrp.gamma * mrp.transitions[s1][s2] * old_v[s2]\n",
    "        if np.sum(np.abs(old_v-new_v)) < thres:\n",
    "            break\n",
    "        old_v = new_v\n",
    "    return new_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write code for Policy Iteration (tabular) algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: evaluate the policy $\\pi$\n",
    "    $$ v_{\\pi}(s) = E[R_{t+1} + \\gamma R_{t+2} + ...|S_t = s]$$\n",
    "    \n",
    "Improve the policy by acting greedily with respect to $v_{\\pi}$.\n",
    "$$ \\pi' = greedy(v_{\\pi})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_policy(mdp: MDP, policy: Mapping[S, Mapping[A, float]], val_func):\n",
    "    mrp = mdp.get_mrp(policy)\n",
    "    lenS = len(mdp.all_states_list)\n",
    "    lenA = len(mdp.all_actions_list)\n",
    "    new_policy = np.zeros(lenS)\n",
    "    for i in range(lenS):\n",
    "        s1 = mdp.all_states_list[i]\n",
    "        qVals = np.zeros(lenA)\n",
    "        for j in range(lenA):\n",
    "            for s2 in mrp.transitions[s1]:\n",
    "                qVals[j] += mrp.rewards[s1] + mrp.gamma * mrp.transitions[s1][s2] * val_func[s2]\n",
    "        new_policy[i] = np.argmax(qVals) \n",
    "    return new_policy\n",
    "    \n",
    "\n",
    "\n",
    "def policy_iter(mdp: MDP):\n",
    "    lenS = len(mdp.all_states_list)\n",
    "    lenA = len(mdp.all_actions_list)\n",
    "    policy_matrix = np.zeros((lenS, lenA))\n",
    "    new_policy = np.zeros((lenS, lenA))\n",
    "    mrp = mdp.get_mrp(policy)\n",
    "    while True:\n",
    "        policy = mdp.matrix_to_policy(policy_matrix)\n",
    "        val_func = policy_eval(mdp, policy, thres)\n",
    "        new_policy = improve_policy(mdp, policy, val_func)\n",
    "        if np.sum(np.abs(new_policy-policy_matrix)) == 0:\n",
    "            break\n",
    "        policy_matrix = new_policy\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write code for Value Iteration (tabular) algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: find optimal policy $\\pi$\n",
    "\n",
    "Solution: iterative application of Bellman optimality backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp: MDP, policy: Mapping[S, Mapping[A, float]], thres = 1e-8):  \n",
    "    mrp = mdp.get_mrp(policy)\n",
    "    lenS = len(mdp.all_states_list)\n",
    "    lenA = len(mdp.all_actions_list)\n",
    "    \n",
    "    old_value = np.zeros(lenS)\n",
    "    new_value = np.zeros(lenS)\n",
    "    while True:\n",
    "        new_value = np.zeros(lenS)\n",
    "        for i in range(lenS):\n",
    "            s1 = mdp.all_states_list[i]\n",
    "            qVals = np.zeros(lenA)           \n",
    "            for j in range(lenA):\n",
    "                for s2 in mrp.transitions[s1]:\n",
    "                    qVals[j] += mrp.rewards[s1] + mrp.gamma * mrp.transitions[s1][s2] * old_value[s2]\n",
    "            \n",
    "            new_value[i] = np.max(qVals)\n",
    "            \n",
    "        if np.sum(np.abs(new_value - old_value)) < thres:\n",
    "            break\n",
    "        old_value = new_value        \n",
    "    return new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
