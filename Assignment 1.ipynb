{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Write out the MP/MRP/MDP/Policy definitions and MRP/MDP Value Function definitions in your own style/notation (so you really internalize these concepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MP:\n",
    "Markov Processes are memoryless random processes.A Markov Process can be represented by a tuple $<S,P>$, where $S$ is a finite set of states and $P$ is a state transition probability matrix. The state transition matrix from $s$ to $s'$ is $P_{SS'}=\\mathbb{P}[S_{t+1}=s'|S_{t}=s]$.\n",
    "\n",
    "##### MRP: \n",
    "A Markov Reward Process is a Markov Process with rewards. It can be represented by a tuple $<S,P,R,\\gamma>$, where $S$ is a finite set of states, $P$ is a state transition probability matrix, $R$ is a reward function, and $\\gamma$ is a discount factor $\\in [0,1]$. The reward function of state $s$ is $R_s=\\mathbb{E}[R_{t+1}|S_t=s]$.\n",
    "\n",
    "##### MDP: \n",
    "A Markov Decision Process is a Markov Reward Process with decisions. It can be represented by a tuple $<S,A,P,R,\\gamma>$, where $S$ is a finite set of states, $A$ is a finite set of actions, $P$ is a state transition probability matrix, $R$ is a reward function, and $\\gamma$ is a discount factor $\\in [0,1]$. The state transition matrix from $s$ with action $a$ to $s'$ is $P_{SS'}^a=\\mathbb{P}[S_{t+1}=s'|S_{t}=s,A_{t}=a]$. The reward function of state $s$ with action $a$ is $R_s^a=\\mathbb{E}[R_{t+1}|S_t=s, A_t=a]$.\n",
    "\n",
    "##### MRP value function: \n",
    "The value function of an MRP is the expected return starting from state $s$.\n",
    "$$v(s) = \\mathbb{E}[G_t|S_t=s]$$\n",
    "where $G_t$ is total discounted reward from $t$.\n",
    "$$G_t = R_{t+1}+\\gamma R_{t+2} + ...= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$$\n",
    "where $R$ is the reward after $k+1$ time steps.\n",
    "\n",
    "##### MDP value function: \n",
    "The value function of an MDP is the expected return starting from state $s$ and then following policy $\\pi$.\n",
    "$$v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t|S_t=s]$$\n",
    "where $\\pi$ is the policy, which is a distribution of actions given states\n",
    "$$\\pi(a|s) = \\mathbb{P}[A_t=a|S_t=s]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Think about the data structures/class design to represent MP/MRP/MDP/Policy/Value Functions and implement them with clear type declarations. \n",
    "Remember - your data structure/code design must resemble the Mathematical/notational formalism as much as possible. \n",
    "Specifically the data structure/code design of MRP/MDP should be incremental (and not independent) to that of MP/MRP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping, Set, Sequence\n",
    "from utils.generic_typevars import S\n",
    "\n",
    "class MP:\n",
    "    \"\"\"A class representing a Markov Process\"\"\"\n",
    "    def __init__(self, transitions: Mapping[S, Mapping[S, float]]):\n",
    "        \"\"\"transitions: a dictionary of dictionaries that stores the transition matrix\"\"\"\n",
    "        self.all_states_list = list(transitions.keys()) # a list that store the names of all states\n",
    "        self.transitions = transitions # a dictionary of dictionaries that stores the transition matrix\n",
    "        \n",
    "    def get_all_states(self):\n",
    "        return self.all_states_list\n",
    "        \n",
    "    def get_transition(self, s: S):\n",
    "        try:\n",
    "            return self.transitions[s]\n",
    "        except ValueError:\n",
    "            print(\"Invalid state!\")\n",
    "            \n",
    "    def \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRP(MP):\n",
    "    \"\"\"A class representing a Markov Reward Process\"\"\"\n",
    "    def __init__(self, transitions: Mapping[S, Mapping[S, float]], rewards, gamma):\n",
    "        super.__init__(self, transitions)\n",
    "        self.rewards = rewards\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def get_reward(self, s: S):\n",
    "        try:\n",
    "            return self.rewards[s]\n",
    "        except ValueError:\n",
    "            print(\"Invalid state!\")\n",
    "        \n",
    "    def get_value_function(self):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP(MRP):\n",
    "    def __init__(self, transitions: Mapping[S, Mapping[S, float]], rewards, gamma, actions):\n",
    "        super.__init__(self, transitions, rewards, gamma)\n",
    "        self.actions = actions\n",
    "        \n",
    "    def get_value_function(self):\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Separately implement the $r(s,s')$ and the $R(s) = \\sum_{s'} p(s,s') * r(s,s')$ definitions of MRP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See functions in class MRP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Write code to convert/cast the r(s,s') definition of MRP to the R(s) definition of MRP (put some thought into code design here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See functions in class MRP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Write code to create a MRP given a MDP and a Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Write out the MDP/MRP Bellman Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Bellman Equation for $v_{\\pi}$:\n",
    "\n",
    "$$ v_{\\pi}(s)=\\sum_{a \\in A} \\pi(a|s)q_{\\pi}(s,a)$$\n",
    "\n",
    "2. Bellman Equation for $q_{\\pi}$:\n",
    "\n",
    "$$ q_{\\pi}(s,a) = R_s^a + \\gamma\\sum_{s' \\in S}P_{ss'}^a v_{\\pi}(s')$$\n",
    "\n",
    "3.  Bellman Equation for $v_{\\pi}$ (2):\n",
    "\n",
    "$$ v_{\\pi}(s)= \\sum_{a \\in A} \\pi(a|s) \\left ( R_s^a + \\gamma\\sum_{s' \\in S}P_{ss'}^a v_{\\pi}(s') \\right )$$\n",
    "\n",
    "4. Bellman Equation for $q_{\\pi}$ (2):\n",
    "\n",
    "$$ q_{\\pi}(s,a) = R_s^a + \\gamma\\sum_{s' \\in S}P_{ss'}^a \\sum_{a' \\in A} \\pi(a'|s')q_{\\pi}(s',a') $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Write code to calculate MRP Value Function (based on Matrix inversion method you learnt in this lecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See functions in class MRP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Write code to generate the stationary distribution for an MP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See functions in class MP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
