{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping, Optional, Set, Callable\n",
    "import numpy as np\n",
    "from utils.generic_typevars import S, A\n",
    "from utils.mp_funcs import get_rv_gen_func_single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prove the Epsilon-Greedy Policy Improvement Theorem (we sketched the proof in Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\begin{split} Q_{\\pi}(s, \\pi'(s)) &= \\sum_{a \\in A} \\pi'(a|s)Q_{\\pi}(s, a) \\\\\n",
    "&= \\frac{\\epsilon}{m}\\sum_{a \\in A} Q_{\\pi}(s,a) + (1-\\epsilon)\\max_{a \\in A}Q_{\\pi} (s,a)\\\\\n",
    "&\\geq \\frac{\\epsilon}{m}\\sum_{a \\in A} Q_{\\pi}(s,a) + (1-\\epsilon)\\sum_{a \\in A} \\frac{\\pi (a|s)-\\frac{\\epsilon}{m}}{1-\\epsilon} Q_{\\pi}(s,a) \\\\\n",
    "&\\geq \\sum_{a \\in A} \\pi(a|s)Q_{\\pi}(s,a) \\\\\n",
    "&= V_{\\pi}(s)\n",
    "\\end{split} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide (with clear mathematical notation) the defintion of GLIE (Greedy in the Limit with Infinite Exploration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All state-action pairs are explored infinitely many times,\n",
    "$$\\lim_{k \\rightarrow \\infty} N_k(s,a) = \\infty $$\n",
    "The policy converges on a greedy policy,\n",
    "$$ \\lim_{k \\rightarrow \\infty } \\pi_k (a|s) = 1(a=\\arg\\max_{a' \\in A} Q_k(s, a')) $$\n",
    "GLIE Monte-Carlo control converges to the optimal action-value function,\n",
    "$$ Q(s,a) \\rightarrow q^*(s, a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the tabular SARSA and tabular SARSA(Lambda) algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDPforRLTab:\n",
    "    '''\n",
    "        First define the MDP class\n",
    "    '''\n",
    "    def __init__(self, policy, actions: Mapping[S, Set[A]], terminal_states: Set[S], state_reward_gen_dict, gamma: float):\n",
    "        self.policy = policy\n",
    "        self.actions = actions\n",
    "        self.terminal_states = terminal_states\n",
    "        self.state_reward_gen_dict = state_reward_gen_dict # a dictionary of functions that generate the next state and reward\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def get_actions(self, s):\n",
    "        return self.actions[s]\n",
    "    \n",
    "    def get_terminal_states(self, s):\n",
    "        return s in self.terminal_states\n",
    "    \n",
    "    def get_state_reward_gen_func(self, s, a):\n",
    "        return self.state_reward_gen_dict[s][a]()\n",
    "    \n",
    "    def init_state_gen(self):\n",
    "        dic = {}\n",
    "        for s in self.actions.keys():\n",
    "            dic[s] = 1. / len(self.actions)\n",
    "        return get_rv_gen_func_single(dic)\n",
    "    \n",
    "    def init_state_action_gen(self):\n",
    "        dic = {}\n",
    "        for s, v1 in self.actions.items():\n",
    "            for a in v1:\n",
    "                dic[(s, a)] = 1. / sum(len(v) for v in self.actions.values())\n",
    "                \n",
    "                \n",
    "class RLTabInterface:\n",
    "    '''\n",
    "    A model-free RL interface that does not need the state-transition probability model or the reward model\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, mdp_for_rl_tab: MDPforRLTab, exploring_start: bool, softmax: bool, epsilon: float, \n",
    "                 epsilon_half_life: float, num_episodes: int, max_steps: int):\n",
    "\n",
    "        self.mdp = mdp_rep_for_rl\n",
    "\n",
    "    # get a state-action dictionary\n",
    "    def get_actions(self) -> Mapping[S, Set[A]]:\n",
    "        return self.mdp.actions\n",
    "    \n",
    "    # check whether a state is a terminal state\n",
    "    def get_terminal_states(self, s) -> bool:\n",
    "        return self.mdp.get_terminal_states(s)\n",
    "    \n",
    "    # get a sampling of the (next state, reward) pair\n",
    "    def get_next_pair(self, s, a):\n",
    "        next_state, reward = self.mdp.get_state_reward_gen_func(s, a)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sarsa:\n",
    "    def __init__(self, mdp: MDPforRLTab, epsilon: float, alpha: float, lamb: float, num_episodes: int, max_steps: int, initial_policy: Policy):\n",
    "        self.mdp = mdp,\n",
    "        self.lamb = lamb\n",
    "        self.epsilon = epsilon,\n",
    "        self.num_episodes = num_episodes,\n",
    "        self.max_steps = max_steps\n",
    "        self.alpha = alpha\n",
    "        self.gamma_lambda = self.mdp.gamma * lamb       \n",
    "        self.policy = initial_policy\n",
    "\n",
    "    def get_qv_func_dict(self, pol: Policy):\n",
    "        control = pol is None\n",
    "        this_pol = pol if pol is not None else self.get_init_policy()\n",
    "        sa_dict = self.mdp_rep.state_action_dict\n",
    "        qf_dict = {s: {a: 0.0 for a in v} for s, v in sa_dict.items()}\n",
    "        episodes = 0\n",
    "        updates = 0\n",
    "\n",
    "        while episodes < self.num_episodes:\n",
    "            if self.exploring_start:\n",
    "                state, action = self.mdp_rep.init_state_action_gen()\n",
    "            else:\n",
    "                state = self.mdp_rep.init_state_gen()\n",
    "                action = get_rv_gen_func_single(\n",
    "                    this_pol.get_state_probabilities(state)\n",
    "                )()\n",
    "            steps = 0\n",
    "            terminate = False\n",
    "\n",
    "            while not terminate:\n",
    "                next_state, reward = self.mdp.state_reward_gen_dict[state][action]()\n",
    "                next_action = get_rv_gen_func_single(this_pol.get_state_probabilities(next_state))()\n",
    "                elif self.algorithm == TDAlgorithm.ExpectedSARSA and control:\n",
    "                    # next_qv = sum(this_pol.get_state_action_probability(\n",
    "                    #     next_state,\n",
    "                    #     a\n",
    "                    # ) * qf_dict[next_state][a] for a in qf_dict[next_state])\n",
    "                    next_qv = get_expected_action_value(\n",
    "                        qf_dict[next_state],\n",
    "                        self.softmax,\n",
    "                        self.epsilon_func(episodes)\n",
    "                    )\n",
    "                else:\n",
    "                    next_qv = qf_dict[next_state][next_action]\n",
    "\n",
    "                qf_dict[state][action] += self.learning_rate *\\\n",
    "                    (updates / self.learning_rate_decay + 1) ** -0.5 *\\\n",
    "                    (reward + self.mdp_rep.gamma * next_qv -\n",
    "                     qf_dict[state][action])\n",
    "                updates += 1\n",
    "                if control:\n",
    "                    if self.softmax:\n",
    "                        this_pol.edit_state_action_to_softmax(\n",
    "                            state,\n",
    "                            qf_dict[state]\n",
    "                        )\n",
    "                    else:\n",
    "                        this_pol.edit_state_action_to_epsilon_greedy(\n",
    "                            state,\n",
    "                            qf_dict[state],\n",
    "                            self.epsilon_func(episodes)\n",
    "                        )\n",
    "                steps += 1\n",
    "                terminate = steps >= self.max_steps or \\\n",
    "                    state in self.mdp_rep.terminal_states\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "\n",
    "            episodes += 1\n",
    "\n",
    "        return qf_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the tabular Q-Learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
