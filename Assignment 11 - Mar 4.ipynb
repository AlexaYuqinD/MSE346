{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Proof (with precise notation) of the Policy Gradient Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin the proof by noting that:\n",
    "    $$ J(\\theta) = \\int_S p_0(s_0) V^{\\pi} (s_0) ds_0 = \\int_S p_0 (s_0) \\int_A \\pi(s_0, a_0; \\theta) Q^{\\pi} (s_0, a_0) da_0 ds_0$$\n",
    "Calculate $\\nabla_{\\theta}J(\\theta)$ by parts $ \\pi(s_0, a_0; \\theta)$ and $ Q^{\\pi} (s_0, a_0)$\n",
    "$$\\nabla_{\\theta}J(\\theta) = \\int_S p_0 (s_0) \\int_A \\nabla_{\\theta} \\pi(s_0, a_0; \\theta) Q^{\\pi} (s_0, a_0) da_0 ds_0 + \\int_S p_0 (s_0) \\int_A \\pi(s_0, a_0; \\theta) \\nabla_{\\theta} Q^{\\pi} (s_0, a_0) da_0 ds_0  $$\n",
    "Using Bellman Equation, $ Q^{\\pi} (s_0, a_0)$ can be expanded to\n",
    "$$ R_{s_0}^{a_0} + \\int_S \\gamma P_{s_0, s_1}^{a_0} V^{\\pi} (s_1) ds_1 $$\n",
    "Therefore, \n",
    "$$ \\nabla_{\\theta}J(\\theta) =  \\int_S p_0 (s_0) \\int_A \\nabla_{\\theta} \\pi(s_0, a_0; \\theta) Q^{\\pi} (s_0, a_0) da_0 ds_0 + \\int_S p_0 (s_0) \\int_A \\pi(s_0, a_0; \\theta) \\nabla_{\\theta} (R_{s_0}^{a_0} + \\int_S \\gamma P_{s_0, s_1}^{a_0} V^{\\pi} (s_1) ds_1 ) da_0 ds_0  $$\n",
    "Since $\\nabla_{\\theta} R_{s_0}^{a_0} = 0$,\n",
    "$$ \\begin{split} \\nabla_{\\theta}J(\\theta) &=  \\int_S p_0 (s_0) \\int_A \\nabla_{\\theta} \\pi(s_0, a_0; \\theta) Q^{\\pi} (s_0, a_0) da_0 ds_0 + \\int_S p_0 (s_0) \\int_A \\pi(s_0, a_0; \\theta) \\nabla_{\\theta}( \\int_S \\gamma P_{s_0, s_1}^{a_0} V^{\\pi} (s_1) ds_1 ) da_0 ds_0 \\\\\n",
    "&= \\int_S p_0 (s_0) \\int_A \\nabla_{\\theta} \\pi(s_0, a_0; \\theta) Q^{\\pi} (s_0, a_0) da_0 ds_0 + \\int_S p_0 (s_0) \\int_A \\pi(s_0, a_0; \\theta) \\int_S \\gamma P_{s_0, s_1}^{a_0} \\nabla_{\\theta} V^{\\pi} (s_1) ds_1 da_0 ds_0 \\\\\n",
    "&= \\int_S p_0 (s_0) \\int_A \\nabla_{\\theta} \\pi(s_0, a_0; \\theta) Q^{\\pi} (s_0, a_0) da_0 ds_0 + \\int_S \\left( \\int_S \\gamma p_0 (s_0)\\int_A \\pi(s_0, a_0; \\theta) P_{s_0, s_1}^{a_0} da_0 ds_0 \\right) \\nabla_{\\theta} V^{\\pi} (s_1) ds_1  \\\\\n",
    "&= \\int_S p_0 (s_0) \\int_A \\nabla_{\\theta} \\pi(s_0, a_0; \\theta) Q^{\\pi} (s_0, a_0) da_0 ds_0 + \\int_S \\left( \\int_S \\gamma p_0 (s_0)p(s_0 \\rightarrow s_1, 1, \\pi) ds_0 \\right) \\nabla_{\\theta} V^{\\pi} (s_1) ds_1  \\\\\n",
    "&= \\int_S p_0 (s_0) \\int_A \\nabla_{\\theta} \\pi(s_0, a_0; \\theta) Q^{\\pi} (s_0, a_0) da_0 ds_0 + \\int_S \\left( \\int_S \\gamma p_0 (s_0)p(s_0 \\rightarrow s_1, 1, \\pi) ds_0 \\right) \\nabla_{\\theta} \\left ( \\int_A \\pi (s_1, a_1; \\theta) Q^{\\pi} (s_1, a_1) da_1\\right ) ds_1  \n",
    "\\end{split}$$\n",
    "Follow the same process of splitting $\\pi Q^{\\pi}$, then Bellman-expanding $Q^{\\pi}$, and iterate\n",
    "$$ \\begin{split} \\nabla_{\\theta}J(\\theta) &=  \\int_S p_0 (s_0) \\int_A \\nabla_{\\theta} \\pi(s_0, a_0; \\theta) Q^{\\pi} (s_0, a_0) da_0 ds_0 + \\int_S \\int_S \\gamma p_0 (s_0)p(s_0 \\rightarrow s_1, 1, \\pi) ds_0  \\left (  \\int_A  \\nabla_{\\theta} \\pi (s_1, a_1; \\theta) Q^{\\pi} (s_1, a_1) da_1 + ...\\right ) ds_1  \\\\\n",
    "&= \\sum_{t=0}^{\\infty} \\int_S \\int_S \\gamma^t p_0 (s_0)p(s_0 \\rightarrow s_t, t, \\pi) ds_0   \\int_A  \\nabla_{\\theta} \\pi (s_t, a_t; \\theta) Q^{\\pi} (s_t, a_t) da_t ds_t  \\\\\n",
    "&=  \\int_S \\int_S \\sum_{t=0}^{\\infty} \\gamma^t p_0 (s_0)p(s_0 \\rightarrow s_t, t, \\pi) ds_0   \\int_A  \\nabla_{\\theta} \\pi (s, a; \\theta) Q^{\\pi} (s, a) da ds  \\end{split} $$\n",
    "Since $\\int_S \\sum_{t=0}^{\\infty} \\gamma^t p_0 (s_0)p(s_0 \\rightarrow s_t, t, \\pi) ds_0  = \\rho^{\\pi} (s)$, we have\n",
    "$$ \\nabla_{\\theta}J(\\theta) = \\int_S  \\rho^{\\pi} (s) \\int_A  \\nabla_{\\theta} \\pi (s, a; \\theta) Q^{\\pi} (s, a) da ds  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive the score function for softmax policy (for finite set of actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\begin{split} \\nabla_{\\theta} \\log{\\pi}(s, a; \\theta) &=\\nabla_{\\theta}  \\log \\frac{e^{\\theta^T}\\phi(s,a)}{\\sum_b e^{\\theta^T}\\phi(s,b)}  \\\\\n",
    "&= \\phi(s,a)-\\sum_b \\pi (s, b; \\theta) \\phi (s,b) \\\\\n",
    "&= \\phi(s,a) - E_{\\pi}[\\phi (s, \\cdot)]  \\end{split} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive the score function for gaussian policy (for continuous actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\begin{split} \\nabla_{\\theta} \\log{\\pi}(s, a; \\theta) = \n",
    "\\frac{(a-\\theta^T \\phi(s))\\phi(s)}{\\sigma^2}  \\end{split} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write code for the REINFORCE Algorithm (Monte-Carlo Policy Gradient Algorithm, i.e., no Critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set, Mapping, Callable, Sequence, Tuple\n",
    "from utils.func_approx_spec import FuncApproxSpec\n",
    "import numpy as np\n",
    "from utils.generic_typevars import S, A\n",
    "\n",
    "class MDPforRLTab:\n",
    "    '''\n",
    "        First define the MDP class\n",
    "    '''\n",
    "    def __init__(self, policy, actions: Mapping[S, Set[A]], terminal_states: Set[S], state_reward_gen_dict, gamma: float):\n",
    "        self.policy = policy\n",
    "        self.actions = actions\n",
    "        self.terminal_states = terminal_states\n",
    "        self.state_reward_gen_dict = state_reward_gen_dict # a dictionary of functions that generate the next state and reward\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def get_actions(self, s):\n",
    "        return self.actions[s]\n",
    "    \n",
    "    def is_terminal_state(self, s):\n",
    "        return s in self.terminal_states\n",
    "    \n",
    "    def get_state_reward_gen_func(self, s, a):\n",
    "        return self.state_reward_gen_dict[s][a]()\n",
    "    \n",
    "    def init_state_gen(self):\n",
    "        dic = {}\n",
    "        for s in self.actions.keys():\n",
    "            dic[s] = 1. / len(self.actions)\n",
    "        return get_rv_gen_func_single(dic)\n",
    "    \n",
    "    def init_state_action_gen(self):\n",
    "        dic = {}\n",
    "        for s, v1 in self.actions.items():\n",
    "            for a in v1:\n",
    "                dic[(s, a)] = 1. / sum(len(v) for v in self.actions.values())\n",
    "                \n",
    "\n",
    "\n",
    "class Reinforce:\n",
    "    def __init__(self, mdp: MDPforRLTab, batch_size: int, num_batches: int, num_action_samples: int, max_steps: int,\n",
    "                score_func: Callable[[A, Sequence[float]], Sequence[float]],\n",
    "                sample_actions_gen_func: Callable[[Sequence[float], int], Sequence[A]], pol_fa_spec: Sequence[FuncApproxSpec]):\n",
    "        self.mdp = mdp\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = num_batches\n",
    "        self.num_action_samples = num_action_samples\n",
    "        self.max_steps = max_steps\n",
    "        self.score_func: Callable[[A, Sequence[float]], Sequence[float]]= score_func\n",
    "        self.sample_actions_gen_func = sample_actions_gen_func\n",
    "        self.pol_fa = [s.get_vf_func_approx_obj() for s in pol_fa_spec]\n",
    "\n",
    "    def get_policy(self):\n",
    "        def pol(s):\n",
    "            def gen_func(samples: int, s=s):\n",
    "                return self.sample_actions_gen_func([f.get_func_eval(s) for f in self.pol_fa],samples)\n",
    "            return gen_func\n",
    "\n",
    "        return pol\n",
    "\n",
    "    def get_path(self, start_state: S):\n",
    "        res = []\n",
    "        state = start_state\n",
    "        steps = 0\n",
    "\n",
    "        while True:\n",
    "            pdf_params = [f.get_func_eval(state) for f in self.pol_fa]\n",
    "            action = self.sample_actions_gen_func(pdf_params, 1)[0]\n",
    "            next_state, reward = self.mdp.state_reward_gen_func(state, action)\n",
    "            res.append((state, pdf_params, action, reward))\n",
    "            steps += 1\n",
    "            if steps >= self.max_steps or self.mdp.is_terminal_state(state):\n",
    "                break\n",
    "            state = next_state\n",
    "        return res\n",
    "\n",
    "    def get_optimal__func(self):\n",
    "        mdp = self.mdp\n",
    "        sc_func = self.score_func\n",
    "\n",
    "        for _ in range(self.num_batches):\n",
    "            pol_grads = [[np.zeros(layer.shape) for layer in this_pol_fa.params] for this_pol_fa in self.pol_fa]\n",
    "            for _ in range(self.batch_size):\n",
    "                states = []\n",
    "                disc_return_scores = []\n",
    "                return_val = 0.\n",
    "                init_state = mdp.init_state_gen_func()\n",
    "                this_path = self.get_path(init_state)\n",
    "\n",
    "                for i, (s, pp, a, r) in enumerate(this_path[::-1]):\n",
    "                    i1 = len(this_path) - i - 1\n",
    "                    states.append(s)\n",
    "                    return_val = return_val * mdp.gamma + r\n",
    "                    disc_return_scores.append(\n",
    "                        [return_val * mdp.gamma ** i1 * x for x in sc_func(a, pp)]\n",
    "                    )\n",
    "\n",
    "                pg_arr = np.vstack(disc_return_scores)\n",
    "                for i, pp_fa in enumerate(self.pol_fa):\n",
    "                    this_pol_grad = pp_fa.get_sum_objective_gradient(states, - pg_arr[:, i])\n",
    "                    for j in range(len(pol_grads[i])):\n",
    "                        pol_grads[i][j] += this_pol_grad[j]\n",
    "\n",
    "            for i, pp_fa in enumerate(self.pol_fa):\n",
    "                pp_fa.update_params_from_gradient([pg / self.batch_size for pg in pol_grads[i]])\n",
    "\n",
    "        return self.get_policy()\n",
    "\n",
    "    def get_optimal_det_policy_func(self) -> Callable[[S], A]:\n",
    "        temp = self.get_optimal__func()\n",
    "        def opt_det_pol_func(s):\n",
    "            return tuple(np.mean(temp(s)(self.num_action_samples), axis=0))\n",
    "        return opt_det_pol_func\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Proof (with proper notation) of the Compatible Function Approximation Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the following two conditions are satisfied:\n",
    "1. Critic gradient is compatible with the Actor score function\n",
    "$$ \\nabla_w Q(s, a; w) = \\nabla_{\\theta} \\log \\pi (s,a; \\theta) \\tag{1}$$\n",
    "2. Critic parameters $w$ minimize the following mean-squared error:\n",
    "$$ \\epsilon = \\int_S \\rho^{\\pi} (s) \\int_A \\pi(s,a; \\theta)(Q^{\\pi})(s,a)-Q(s,a;w))^2 da \\cdot ds \\tag{2}$$\n",
    "\n",
    "Then the Policy Gradient using critic $Q(s,a;w)$ is exact\n",
    "$$ \\nabla_{\\theta} J(\\theta) = \\int_S \\rho^{\\pi} (s) \\int_A \\nabla_{\\theta} \\pi (s,a; \\theta) Q(s,a; w) da \\cdot ds \\tag{3}$$\n",
    "\n",
    "For $w$ that minimizes (2),\n",
    "\n",
    "$$ \\int_S \\rho^{\\pi} (s) \\int_A \\pi(s,a; \\theta)(Q^{\\pi})(s,a)-Q(s,a;w))\\nabla_{w} Q(s,a; w)  da \\cdot ds = 0$$\n",
    "\n",
    "Combined with (1), we have \n",
    "\n",
    "$$\\int_S \\rho^{\\pi} (s) \\int_A \\pi(s,a; \\theta)(Q^{\\pi})(s,a)-Q(s,a;w))\\nabla_{\\theta} \\log \\pi (s,a; \\theta) da \\cdot ds = 0  $$\n",
    "\n",
    "Therefore, \n",
    "\n",
    "$$\\int_S \\rho^{\\pi} (s) \\int_A \\pi(s,a; \\theta)Q^{\\pi}(s,a)\\nabla_{\\theta} \\log \\pi (s,a; \\theta) da \\cdot ds = \\int_S \\rho^{\\pi} (s) \\int_A \\pi(s,a; \\theta)Q(s,a;w)\\nabla_{\\theta} \\log \\pi (s,a; \\theta) da \\cdot ds  $$\n",
    "\n",
    "Combined with (3), we have\n",
    "$$ \\nabla_{\\theta} J(\\theta) = \\int_S \\rho^{\\pi} (s) \\int_A \\nabla_{\\theta} \\pi(s,a; \\theta)Q(s,a;w) da \\cdot ds  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
