{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Write out the MP/MRP/MDP/Policy definitions and MRP/MDP Value Function definitions in your own style/notation (so you really internalize these concepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MP:\n",
    "Markov Processes are memoryless random processes.A Markov Process can be represented by a tuple $<S,P>$, where $S$ is a finite set of states and $P$ is a state transition probability matrix. The state transition matrix from $s$ to $s'$ is $P_{SS'}=\\mathbb{P}[S_{t+1}=s'|S_{t}=s]$.\n",
    "\n",
    "##### MRP: \n",
    "A Markov Reward Process is a Markov Process with rewards. It can be represented by a tuple $<S,P,R,\\gamma>$, where $S$ is a finite set of states, $P$ is a state transition probability matrix, $R$ is a reward function, and $\\gamma$ is a discount factor $\\in [0,1]$. The reward function of state $s$ is $R_s=\\mathbb{E}[R_{t+1}|S_t=s]$.\n",
    "\n",
    "##### MDP: \n",
    "A Markov Decision Process is a Markov Reward Process with decisions. It can be represented by a tuple $<S,A,P,R,\\gamma>$, where $S$ is a finite set of states, $A$ is a finite set of actions, $P$ is a state transition probability matrix, $R$ is a reward function, and $\\gamma$ is a discount factor $\\in [0,1]$. The state transition matrix from $s$ with action $a$ to $s'$ is $P_{SS'}^a=\\mathbb{P}[S_{t+1}=s'|S_{t}=s,A_{t}=a]$. The reward function of state $s$ with action $a$ is $R_s^a=\\mathbb{E}[R_{t+1}|S_t=s, A_t=a]$.\n",
    "\n",
    "##### MRP value function: \n",
    "The value function of an MRP is the expected return starting from state $s$.\n",
    "$$v(s) = \\mathbb{E}[G_t|S_t=s]$$\n",
    "where $G_t$ is total discounted reward from $t$.\n",
    "$$G_t = R_{t+1}+\\gamma R_{t+2} + ...= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$$\n",
    "where $R$ is the reward after $k+1$ time steps.\n",
    "\n",
    "##### MDP value function: \n",
    "The value function of an MDP is the expected return starting from state $s$ and then following policy $\\pi$.\n",
    "$$v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t|S_t=s]$$\n",
    "where $\\pi$ is the policy, which is a distribution of actions given states\n",
    "$$\\pi(a|s) = \\mathbb{P}[A_t=a|S_t=s]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Think about the data structures/class design to represent MP/MRP/MDP/Policy/Value Functions and implement them with clear type declarations. \n",
    "Remember - your data structure/code design must resemble the Mathematical/notational formalism as much as possible. \n",
    "Specifically the data structure/code design of MRP/MDP should be incremental (and not independent) to that of MP/MRP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping, Set, Sequence\n",
    "from utils.generic_typevars import S, A\n",
    "import numpy as np\n",
    "\n",
    "class MP:\n",
    "    \"\"\"A class representing a Markov Process\"\"\"\n",
    "    def __init__(self, transitions: Mapping[S, Mapping[S, float]]):\n",
    "        \"\"\"transitions: a dictionary of dictionaries that stores the transition matrix\"\"\"\n",
    "        self.all_states_list = list(transitions.keys()) # a list that store the names of all states\n",
    "        self.transitions = transitions # a dictionary of dictionaries that stores the transition matrix\n",
    "        \n",
    "    def get_all_states(self):\n",
    "        return self.all_states_list\n",
    "        \n",
    "    def get_transition(self, s: S):\n",
    "        try:\n",
    "            return self.transitions[s]\n",
    "        except ValueError:\n",
    "            print(\"Invalid state!\")\n",
    "            \n",
    "    def get_trans_mat(self):\n",
    "        length = len(self.all_states_list)\n",
    "        trans_mat = np.zeros((length, length))\n",
    "        for i in range(length):\n",
    "            for j in range(length):\n",
    "                trans_mat[i, j] = self.transitions[self.all_states_list[i]].get(self.all_states_list[j], 0)\n",
    "        return trans_mat\n",
    "            \n",
    "    def get_stationary_dist(self):\n",
    "        trans_mat = self.get_trans_mat()\n",
    "        eig_val, eig_vec = np.linalg.eig(trans_mat.T)\n",
    "        res = np.array(eig_vec[:, np.where(np.abs(eig_val- 1.) < 1e-8)[0][0]])\n",
    "        res /= np.sum(res)    \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRP(MP):\n",
    "    \"\"\"A class representing a Markov Reward Process\"\"\"\n",
    "    def __init__(self, transitions: Mapping[S, Mapping[S, float]], rewards: Mapping[S, float], gamma: float):\n",
    "        super().__init__(transitions)\n",
    "        self.rewards = rewards\n",
    "        self.reward_vec = self.get_rewards_vec()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def get_reward(self, s: S):\n",
    "        try:\n",
    "            return self.rewards[s]\n",
    "        except ValueError:\n",
    "            print(\"Invalid state!\")\n",
    "            \n",
    "    def get_rewards_vec(self):\n",
    "        length = len(self.all_states_list)\n",
    "        reward_vec = np.zeros(length)\n",
    "        for i in range(length):\n",
    "            reward_vec[i] = self.rewards[self.all_states_list[i]]\n",
    "        return reward_vec             \n",
    "\n",
    "    def get_value_function(self):\n",
    "        trans_mat = self.get_trans_mat()\n",
    "        return np.linalg.inv(np.eye(len(self.all_states_list)) - self.gamma * trans_mat).dot(self.reward_vec)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    \"\"\"A class representing a Markov Decision Process\"\"\"\n",
    "    def __init__(self, transitions: Mapping[S, Mapping[A, Mapping[S, float]]], rewards: Mapping[S, Mapping[A, float]], gamma):\n",
    "        self.all_states_list = list(transitions.keys()) \n",
    "        self.transitions = transitions\n",
    "        self.actions = self.get_actions()\n",
    "        self.rewards = rewards\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def get_all_states(self):\n",
    "        return self.all_states_list\n",
    "    \n",
    "    def get_actions(self) ->  Mapping[S, Set[A]]:\n",
    "        action_dict = {}\n",
    "        for s in self.all_states_list:\n",
    "            action_dict[s] = set()\n",
    "        for s1, v1 in self.transitions.items():\n",
    "            for a, v2 in v1.items():\n",
    "                action_dict[s1].add(a)\n",
    "        return action_dict\n",
    "        \n",
    "    def get_transition(self, s: S, a: A):\n",
    "        try:\n",
    "            return self.transitions[s][a]\n",
    "        except ValueError:\n",
    "            print(\"Invalid state!\")\n",
    "            \n",
    "    def get_mrp(self, policy: Mapping[S, Mapping[A, float]]):\n",
    "        transitions = {}\n",
    "        rewards = {}\n",
    "        for s1, v1 in self.transitions.items():\n",
    "            transitions[s1] = {}\n",
    "            for a, p in policy[s1].items():\n",
    "                for s2, v2 in v1[a].items():\n",
    "                    transitions[s1][s2] = transitions[s1].get(s2, 0) + p*v2\n",
    "                    \n",
    "        for s1, v1 in self.rewards.items():\n",
    "            rewards[s1] = 0\n",
    "            for a, p in policy[s1].items():\n",
    "                rewards[s1] += p*v1[a]\n",
    "        \n",
    "        return MRP(transitions, rewards, self.gamma)\n",
    "    \n",
    "    def get_value_function_dict(self, policy: Mapping[S, Mapping[A, float]]):\n",
    "        mrp = self.get_mrp(policy)\n",
    "        val_func_vec = mrp.get_value_function()\n",
    "        length = len(mrp.all_states_list)\n",
    "        val_func_dict = {}\n",
    "        for s in self.all_states_list:\n",
    "            val_func_dict[s] = 0\n",
    "        for i in range(length):\n",
    "            val_func_dict[mrp.all_states_list[i]] = val_func_vec[i]\n",
    "        return val_func_dict\n",
    "    \n",
    "    def get_action_value_function(self, policy: Mapping[S, Mapping[A, float]]):\n",
    "        val_func_dict = self.get_value_function_dict(policy)\n",
    "        action_val_func_dict = {}\n",
    "        for s1, v1 in self.rewards.items():\n",
    "            action_val_func_dict[s1] = {}\n",
    "            for a, v2 in v1.items():\n",
    "                action_val_func_dict[s1][a] = v2 \n",
    "                for s2, p in self.transitions[s1][a].items():                 \n",
    "                    action_val_func_dict[s1][a] += self.gamma * p * val_func_dict[s2]\n",
    "        return action_val_func_dict\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Separately implement the $r(s,s')$ and the $R(s) = \\sum_{s'} p(s,s') * r(s,s')$ definitions of MRP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See get_reward() and get_value_function() functions in class MRP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Write code to convert/cast the r(s,s') definition of MRP to the R(s) definition of MRP (put some thought into code design here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_Reward(reward: Mapping[S, Mapping[S, float]], trans: Mapping[S, Mapping[S, float]]) -> Mapping[S, float]:  \n",
    "    rs = {}\n",
    "    for s1 in trans.keys():\n",
    "        rs[s1] = 0\n",
    "        for s2, prob in trans[s].items():\n",
    "            rs[s1] += prob * r_s[s1][s2]    \n",
    "    return rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Write code to create a MRP given a MDP and a Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created an MRP. \n",
      "\n",
      "All states:\n",
      "[1, 2, 3]\n",
      "\n",
      "Transition matrix:\n",
      "[[0.03 0.25 0.45]\n",
      " [0.35 0.45 0.2 ]\n",
      " [0.4  0.48 0.12]]\n",
      "\n",
      "Rewards:\n",
      "[3.76 0.4  2.44]\n",
      "\n",
      "Value function:\n",
      "[11.48997194 10.52806415 12.47142781]\n"
     ]
    }
   ],
   "source": [
    "transitions = {\n",
    "    1: {\n",
    "        'a': {1: 0.3, 2: 0.7},\n",
    "        'b': {2: 0.3, 2: 0.2, 3: 0.5},\n",
    "    },\n",
    "    2: {\n",
    "        'a': {1: 0.4, 2: 0.6},\n",
    "        'b': {1: 0.3, 2: 0.3, 3: 0.4}\n",
    "    },\n",
    "    3: {\n",
    "        'a': {1: 1.0},\n",
    "        'b': {2: 0.8, 3: 0.2}\n",
    "    }\n",
    "}\n",
    "\n",
    "rewards = {\n",
    "    1: {\n",
    "        'a': 5.2,\n",
    "        'b': 3.6\n",
    "    },\n",
    "    2: {\n",
    "        'a': 1.8,\n",
    "        'b': -1.0\n",
    "    },\n",
    "    3: {\n",
    "        'a': 0.1,\n",
    "        'b': 4.0\n",
    "    }\n",
    "}\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "mdp = MDP(transitions, rewards, gamma)\n",
    "\n",
    "\n",
    "policy = {\n",
    "    1: {\n",
    "        'a': 0.1,\n",
    "        'b': 0.9\n",
    "    },\n",
    "    2: {\n",
    "        'a': 0.5,\n",
    "        'b': 0.5\n",
    "    },\n",
    "    3: {\n",
    "        'a': 0.4,\n",
    "        'b': 0.6\n",
    "    }\n",
    "}\n",
    "\n",
    "mrp = mdp.get_mrp(policy)\n",
    "print(\"Successfully created an MRP. \\n\")\n",
    "print(\"All states:\")\n",
    "print(mrp.get_all_states())\n",
    "print(\"\\nTransition matrix:\")\n",
    "print(mrp.get_trans_mat())\n",
    "print(\"\\nRewards:\")\n",
    "print(mrp.get_rewards_vec())\n",
    "print(\"\\nValue function:\")\n",
    "print(mrp.get_value_function())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Write out the MDP/MRP Bellman Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Bellman Equation for $v_{\\pi}$:\n",
    "\n",
    "$$ v_{\\pi}(s)=\\sum_{a \\in A} \\pi(a|s)q_{\\pi}(s,a)$$\n",
    "\n",
    "2. Bellman Equation for $q_{\\pi}$:\n",
    "\n",
    "$$ q_{\\pi}(s,a) = R_s^a + \\gamma\\sum_{s' \\in S}P_{ss'}^a v_{\\pi}(s')$$\n",
    "\n",
    "3.  Bellman Equation for $v_{\\pi}$ (2):\n",
    "\n",
    "$$ v_{\\pi}(s)= \\sum_{a \\in A} \\pi(a|s) \\left ( R_s^a + \\gamma\\sum_{s' \\in S}P_{ss'}^a v_{\\pi}(s') \\right )$$\n",
    "\n",
    "4. Bellman Equation for $q_{\\pi}$ (2):\n",
    "\n",
    "$$ q_{\\pi}(s,a) = R_s^a + \\gamma\\sum_{s' \\in S}P_{ss'}^a \\sum_{a' \\in A} \\pi(a'|s')q_{\\pi}(s',a') $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Write code to calculate MRP Value Function (based on Matrix inversion method you learnt in this lecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See get_value_function() function in class MRP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Write code to generate the stationary distribution for an MP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See get_stationary_dist() function in class MP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
