{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping, Optional, Set, Callable\n",
    "import numpy as np\n",
    "from utils.generic_typevars import S, A\n",
    "from utils.mp_funcs import get_rv_gen_func_single\n",
    "from utils.mp_funcs import get_expected_action_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prove the Epsilon-Greedy Policy Improvement Theorem (we sketched the proof in Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\begin{split} Q_{\\pi}(s, \\pi'(s)) &= \\sum_{a \\in A} \\pi'(a|s)Q_{\\pi}(s, a) \\\\\n",
    "&= \\frac{\\epsilon}{m}\\sum_{a \\in A} Q_{\\pi}(s,a) + (1-\\epsilon)\\max_{a \\in A}Q_{\\pi} (s,a)\\\\\n",
    "&\\geq \\frac{\\epsilon}{m}\\sum_{a \\in A} Q_{\\pi}(s,a) + (1-\\epsilon)\\sum_{a \\in A} \\frac{\\pi (a|s)-\\frac{\\epsilon}{m}}{1-\\epsilon} Q_{\\pi}(s,a) \\\\\n",
    "&\\geq \\sum_{a \\in A} \\pi(a|s)Q_{\\pi}(s,a) \\\\\n",
    "&= V_{\\pi}(s)\n",
    "\\end{split} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide (with clear mathematical notation) the defintion of GLIE (Greedy in the Limit with Infinite Exploration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All state-action pairs are explored infinitely many times,\n",
    "$$\\lim_{k \\rightarrow \\infty} N_k(s,a) = \\infty $$\n",
    "The policy converges on a greedy policy,\n",
    "$$ \\lim_{k \\rightarrow \\infty } \\pi_k (a|s) = 1(a=\\arg\\max_{a' \\in A} Q_k(s, a')) $$\n",
    "GLIE Monte-Carlo control converges to the optimal action-value function,\n",
    "$$ Q(s,a) \\rightarrow q^*(s, a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the tabular SARSA and tabular SARSA(Lambda) algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDPforRLTab:\n",
    "    '''\n",
    "        First define the MDP class\n",
    "    '''\n",
    "    def __init__(self, policy, actions: Mapping[S, Set[A]], terminal_states: Set[S], state_reward_gen_dict, gamma: float):\n",
    "        self.policy = policy\n",
    "        self.actions = actions\n",
    "        self.terminal_states = terminal_states\n",
    "        self.state_reward_gen_dict = state_reward_gen_dict # a dictionary of functions that generate the next state and reward\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def get_actions(self, s):\n",
    "        return self.actions[s]\n",
    "    \n",
    "    def get_terminal_states(self, s):\n",
    "        return s in self.terminal_states\n",
    "    \n",
    "    def get_state_reward_gen_func(self, s, a):\n",
    "        return self.state_reward_gen_dict[s][a]()\n",
    "    \n",
    "    def init_state_gen(self):\n",
    "        dic = {}\n",
    "        for s in self.actions.keys():\n",
    "            dic[s] = 1. / len(self.actions)\n",
    "        return get_rv_gen_func_single(dic)\n",
    "    \n",
    "    def init_state_action_gen(self):\n",
    "        dic = {}\n",
    "        for s, v1 in self.actions.items():\n",
    "            for a in v1:\n",
    "                dic[(s, a)] = 1. / sum(len(v) for v in self.actions.values())\n",
    "                \n",
    "                \n",
    "class RLTabInterface:\n",
    "    '''\n",
    "    A model-free RL interface that does not need the state-transition probability model or the reward model\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, mdp: MDPforRLTab, exploring_start: bool, softmax: bool, epsilon: float, \n",
    "                 epsilon_half_life: float, num_episodes: int, max_steps: int):\n",
    "\n",
    "        self.mdp = mdp\n",
    "\n",
    "    # get a state-action dictionary\n",
    "    def get_actions(self) -> Mapping[S, Set[A]]:\n",
    "        return self.mdp.actions\n",
    "    \n",
    "    # check whether a state is a terminal state\n",
    "    def get_terminal_states(self, s) -> bool:\n",
    "        return self.mdp.get_terminal_states(s)\n",
    "    \n",
    "    # get a sampling of the (next state, reward) pair\n",
    "    def get_next_pair(self, s, a):\n",
    "        next_state, reward = self.mdp.get_state_reward_gen_func(s, a)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sarsa:\n",
    "    def __init__(self, mdp: MDPforRLTab, epsilon: float, alpha: float, lamb: float, num_episodes: int, max_steps: int, learning_rate_decay: float):\n",
    "        self.mdp = mdp,\n",
    "        self.lamb = lamb\n",
    "        self.epsilon = epsilon,\n",
    "        self.num_episodes = num_episodes,\n",
    "        self.max_steps = max_steps\n",
    "        self.alpha = alpha      \n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "\n",
    "    def get_q_values(self, pol):\n",
    "        policy = pol if pol is not None else self.get_init_policy()\n",
    "        q_values = {s: {a: 0.0 for a in v} for s, v in self.mdp.state_action_dict.items()}\n",
    "        episodes = 0\n",
    "        updates = 0\n",
    "\n",
    "        while episodes < self.num_episodes:\n",
    "            state = self.mdp.init_state_gen()\n",
    "            action = get_rv_gen_func_single(policy.get_state_probabilities(state))()\n",
    "            steps = 0\n",
    "\n",
    "            while True:\n",
    "                next_state, reward = self.mdp.state_reward_gen_dict[state][action]()\n",
    "                next_action = get_rv_gen_func_single(policy.get_state_probabilities(next_state))()\n",
    "                next_q = get_expected_action_value(q_values[next_state], False, self.epsilon)\n",
    "                q_values[state][action] += self.alpha*(updates / self.learning_rate_decay + 1) ** -0.5 *\\\n",
    "                    (reward + self.mdp.gamma * next_q - q_values[state][action])\n",
    "                updates += 1\n",
    "                steps += 1\n",
    "                if steps >= self.max_steps or state in self.mdp.terminal_states:\n",
    "                    break\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "\n",
    "            episodes += 1\n",
    "\n",
    "        return q_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the tabular Q-Learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(self, mdp: MDPforRLTab, epsilon: float, alpha: float, lamb: float, num_episodes: int, max_steps: int, learning_rate_decay: float):\n",
    "        self.mdp = mdp,\n",
    "        self.lamb = lamb\n",
    "        self.epsilon = epsilon,\n",
    "        self.num_episodes = num_episodes,\n",
    "        self.max_steps = max_steps\n",
    "        self.alpha = alpha      \n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "\n",
    "    def get_q_values(self, pol):\n",
    "        policy = pol if pol is not None else self.get_init_policy()\n",
    "        q_values = {s: {a: 0.0 for a in v} for s, v in self.mdp.state_action_dict.items()}\n",
    "        episodes = 0\n",
    "        updates = 0\n",
    "\n",
    "        while episodes < self.num_episodes:\n",
    "            state = self.mdp.init_state_gen()\n",
    "            action = get_rv_gen_func_single(policy.get_state_probabilities(state))()\n",
    "            steps = 0\n",
    "\n",
    "            while True:\n",
    "                next_state, reward = self.mdp.state_reward_gen_dict[state][action]()\n",
    "                next_action = get_rv_gen_func_single(policy.get_state_probabilities(next_state))()\n",
    "                next_q = max(q_values[next_state][a] for a in q_values[next_state])\n",
    "                q_values[state][action] += self.alpha*(updates / self.learning_rate_decay + 1) ** -0.5 *\\\n",
    "                    (reward + self.mdp.gamma * next_q - q_values[state][action])\n",
    "                updates += 1\n",
    "                steps += 1\n",
    "                if steps >= self.max_steps or state in self.mdp.terminal_states:\n",
    "                    break\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "\n",
    "            episodes += 1\n",
    "\n",
    "        return q_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
